{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNAq3STj7eisT1G/++g0pBD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hamshika1/neural-language-model/blob/main/neural_language_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NwBgvYG21JgL"
      },
      "outputs": [],
      "source": [
        "!mkdir nlm-assignment\n",
        "!mkdir nlm-assignment/data\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp Pride_and_Prejudice-Jane_Austen.txt nlm-assignment/data/dataset.txt\n"
      ],
      "metadata": {
        "id": "GlIcislQ2Cgx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls nlm-assignment/data\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rh-9mTpM2HJu",
        "outputId": "dcf013dc-05cb-4ad3-b1ed-f9da41512577"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dataset.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch matplotlib\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qsbxBXaP2Kxb",
        "outputId": "01dc1080-7184-4b08-f8ae-81d2a825346a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (3.10.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (4.60.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.4.9)\n",
            "Requirement already satisfied: numpy>=1.23 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (3.2.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sUSvbbCr3OBm",
        "outputId": "a564d0fc-81b0-46a1-bb11-ba78aaaf7a85"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hAVrBmPZ3S6J",
        "outputId": "2a5c46bb-2ec8-4081-8cba-182aeb6e2fcc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nlm-assignment\tPride_and_Prejudice-Jane_Austen.txt  sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p nlm-assignment/data\n",
        "!cp Pride_and_Prejudice-Jane_Austen.txt nlm-assignment/data/dataset.txt\n"
      ],
      "metadata": {
        "id": "OuR4vGkg3mHa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls nlm-assignment/data\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dQswPnS63pxw",
        "outputId": "425bb9a9-6287-4ef2-9c0a-18fba67114c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dataset.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd nlm-assignment"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lldZTu5I3teD",
        "outputId": "5434c5c2-423f-449f-83f3-c4535270ca80"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/nlm-assignment\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mx7cAE4K3zE8",
        "outputId": "efb5bc83-7c9f-4242-ae35-645750ba2ae2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data  train_lm.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "print(torch.cuda.is_available())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z0zVzeB04Ael",
        "outputId": "0c0e1339-74ca-4bf3-de7f-f16fb44d6493"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "print(torch.cuda.is_available())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IXL9B9JE4XrT",
        "outputId": "be84e6c8-892f-47ef-b478-101385363b65"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/nlm-assignment"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pGfqgNRr4cHo",
        "outputId": "927ef48c-0e21-4e54-86bf-6b4349a9dfa2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Errno 2] No such file or directory: '/content/nlm-assignment'\n",
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ouBqhNZ44kAo",
        "outputId": "662b1e3d-4c5c-492e-b94b-caf75e446f71"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DvFyTqWE4rAD",
        "outputId": "9f4035d1-36be-4b1c-8d25-78e4618b20a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls data\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jlGVuDzE4uoo",
        "outputId": "6cb4aff2-a9ee-472f-d3fe-cb0227cb1495"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ls: cannot access 'data': No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir nlm-assignment\n",
        "!mkdir nlm-assignment/data"
      ],
      "metadata": {
        "id": "t7mGnrZB4ue-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp nlm-assignment/Pride_and_Prejudice-Jane_Austen.txt nlm-assignment/data/dataset.txt\n"
      ],
      "metadata": {
        "id": "TS_0s7_Z5nvO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls nlm-assignment/data\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QBrWDkHF5rE-",
        "outputId": "4f427a2e-abb8-40d7-fb9d-09a40b65fa31"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dataset.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/nlm-assignment"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y5ak-CK55vo7",
        "outputId": "732b116c-6108-4e75-8a25-0ce83182c3de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/nlm-assignment\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZTZIH0a05yb4",
        "outputId": "7b4f6f0a-90f4-40fc-dc10-3c4c6fa610d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/nlm-assignment\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import math\n",
        "import random\n",
        "import time\n",
        "import json\n",
        "from collections import Counter\n",
        "from typing import List\n",
        "\n",
        "import matplotlib\n",
        "matplotlib.use(\"Agg\")\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# ---------------------------------------\n",
        "# GLOBAL SETTINGS\n",
        "# ---------------------------------------\n",
        "SEED = 12345\n",
        "random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Using device:\", DEVICE)\n",
        "\n",
        "DATA_PATH = \"data/dataset.txt\"\n",
        "OUT_DIR = \"outputs\"\n",
        "\n",
        "# ---------------------------------------\n",
        "# TOKENIZATION + VOCAB\n",
        "# ---------------------------------------\n",
        "class Vocab:\n",
        "    def __init__(self, min_freq=1):\n",
        "        self.specials = [\"<pad>\", \"<unk>\", \"<eos>\"]\n",
        "        self.min_freq = min_freq\n",
        "        self.stoi = {}\n",
        "        self.itos = []\n",
        "\n",
        "    def build(self, token_list):\n",
        "        counter = Counter(token_list)\n",
        "        words = [w for w, c in counter.items() if c >= self.min_freq]\n",
        "        self.itos = self.specials + words\n",
        "        self.stoi = {w: i for i, w in enumerate(self.itos)}\n",
        "\n",
        "        self.pad = self.stoi[\"<pad>\"]\n",
        "        self.unk = self.stoi[\"<unk>\"]\n",
        "        self.eos = self.stoi[\"<eos>\"]\n",
        "\n",
        "    def encode(self, tokens):\n",
        "        return [self.stoi.get(t, self.unk) for t in tokens]\n",
        "\n",
        "# ---------------------------------------\n",
        "# DATASET\n",
        "# ---------------------------------------\n",
        "class LMDataset(Dataset):\n",
        "    def __init__(self, ids, seq_len):\n",
        "        self.ids = ids\n",
        "        self.seq_len = seq_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.ids) - self.seq_len\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        x = self.ids[idx: idx + self.seq_len]\n",
        "        y = self.ids[idx + 1: idx + 1 + self.seq_len]\n",
        "        return torch.tensor(x), torch.tensor(y)\n",
        "\n",
        "# ---------------------------------------\n",
        "# MODELS\n",
        "# ---------------------------------------\n",
        "class LSTMLM(nn.Module):\n",
        "    def __init__(self, vocab_size, emb=128, hid=128, layers=2, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.embed = nn.Embedding(vocab_size, emb)\n",
        "        self.lstm = nn.LSTM(emb, hid, layers, batch_first=True, dropout=dropout)\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "        self.fc = nn.Linear(hid, vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        e = self.embed(x)\n",
        "        o, _ = self.lstm(e)\n",
        "        o = self.drop(o)\n",
        "        return self.fc(o)\n",
        "\n",
        "class TransformerLM(nn.Module):\n",
        "    def __init__(self, vocab_size, emb=256, heads=8, layers=4, ff=512, dropout=0.1, max_len=512):\n",
        "        super().__init__()\n",
        "        self.emb = nn.Embedding(vocab_size, emb)\n",
        "        self.pos = nn.Embedding(max_len, emb)\n",
        "\n",
        "        enc = nn.TransformerEncoderLayer(\n",
        "            d_model=emb,\n",
        "            nhead=heads,\n",
        "            dim_feedforward=ff,\n",
        "            dropout=dropout\n",
        "        )\n",
        "        self.enc = nn.TransformerEncoder(enc, num_layers=layers)\n",
        "        self.fc = nn.Linear(emb, vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, s = x.shape\n",
        "        pos = torch.arange(s, device=x.device).unsqueeze(0).expand(b, s)\n",
        "        e = self.emb(x) + self.pos(pos)\n",
        "        e = e.transpose(0, 1)\n",
        "\n",
        "        mask = torch.triu(torch.full((s, s), float(\"-inf\"), device=x.device), diagonal=1)\n",
        "        o = self.enc(e, mask=mask)\n",
        "        o = o.transpose(0, 1)\n",
        "        return self.fc(o)\n",
        "\n",
        "# ---------------------------------------\n",
        "# TRAINING + EVAL\n",
        "# ---------------------------------------\n",
        "def evaluate(model, loader, crit):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    total_tokens = 0\n",
        "    with torch.no_grad():\n",
        "        for x, y in loader:\n",
        "            x, y = x.to(DEVICE), y.to(DEVICE)\n",
        "            logits = model(x)\n",
        "            loss = crit(logits.view(-1, logits.size(-1)), y.view(-1))\n",
        "            total_loss += loss.item() * x.numel()\n",
        "            total_tokens += x.numel()\n",
        "    avg_loss = total_loss / total_tokens\n",
        "    return avg_loss, math.exp(avg_loss)\n",
        "\n",
        "def train_epoch(model, loader, optim, crit):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    total_tokens = 0\n",
        "    for x, y in loader:\n",
        "        x, y = x.to(DEVICE), y.to(DEVICE)\n",
        "        optim.zero_grad()\n",
        "        logits = model(x)\n",
        "        loss = crit(logits.view(-1, logits.size(-1)), y.view(-1))\n",
        "        loss.backward()\n",
        "        optim.step()\n",
        "        total_loss += loss.item() * x.numel()\n",
        "        total_tokens += x.numel()\n",
        "    avg_loss = total_loss / total_tokens\n",
        "    return avg_loss, math.exp(avg_loss)\n",
        "\n",
        "# ---------------------------------------\n",
        "# RUN EXPERIMENT (one config)\n",
        "# ---------------------------------------\n",
        "def run_experiment(name, config):\n",
        "\n",
        "    print(f\"\\n=== Running: {name} ===\")\n",
        "\n",
        "    # Load dataset\n",
        "    with open(DATA_PATH, \"r\", encoding=\"utf-8\") as f:\n",
        "        lines = [l.strip().split() + [\"<eos>\"] for l in f.readlines()]\n",
        "\n",
        "    all_tokens = [t for line in lines for t in line]\n",
        "\n",
        "    # Build vocab\n",
        "    vocab = Vocab(min_freq=config[\"min_freq\"])\n",
        "    vocab.build(all_tokens)\n",
        "\n",
        "    # Convert to ids\n",
        "    ids = vocab.encode(all_tokens)\n",
        "\n",
        "    # Split train/val/test\n",
        "    n = len(ids)\n",
        "    train_ids = ids[: int(0.8*n)]\n",
        "    val_ids = ids[int(0.8*n): int(0.9*n)]\n",
        "    test_ids = ids[int(0.9*n):]\n",
        "\n",
        "    # Datasets\n",
        "    train_ds = LMDataset(train_ids, config[\"seq_len\"])\n",
        "    val_ds   = LMDataset(val_ids,   config[\"seq_len\"])\n",
        "    test_ds  = LMDataset(test_ids,  config[\"seq_len\"])\n",
        "\n",
        "    train_ld = DataLoader(train_ds, batch_size=config[\"batch\"], shuffle=True)\n",
        "    val_ld   = DataLoader(val_ds,   batch_size=config[\"batch\"])\n",
        "    test_ld  = DataLoader(test_ds,  batch_size=config[\"batch\"])\n",
        "\n",
        "    # Model\n",
        "    if config[\"model\"] == \"lstm\":\n",
        "        model = LSTMLM(len(vocab.itos),\n",
        "                       emb=config[\"emb\"],\n",
        "                       hid=config[\"hid\"],\n",
        "                       layers=config[\"layers\"])\n",
        "    else:\n",
        "        model = TransformerLM(len(vocab.itos),\n",
        "                              emb=config[\"emb\"],\n",
        "                              heads=config[\"heads\"],\n",
        "                              layers=config[\"layers\"],\n",
        "                              ff=config[\"ff\"])\n",
        "\n",
        "    model = model.to(DEVICE)\n",
        "    crit = nn.CrossEntropyLoss()\n",
        "    optim = torch.optim.Adam(model.parameters(), lr=config[\"lr\"])\n",
        "\n",
        "    # Training\n",
        "    history = {\"train\": [], \"val\": []}\n",
        "\n",
        "    os.makedirs(f\"{OUT_DIR}/{name}\", exist_ok=True)\n",
        "\n",
        "    best_val = float(\"inf\")\n",
        "\n",
        "    for epoch in range(1, config[\"epochs\"] + 1):\n",
        "        tr_loss, tr_ppl = train_epoch(model, train_ld, optim, crit)\n",
        "        va_loss, va_ppl = evaluate(model, val_ld, crit)\n",
        "\n",
        "        history[\"train\"].append(tr_loss)\n",
        "        history[\"val\"].append(va_loss)\n",
        "\n",
        "        print(f\"Epoch {epoch}/{config['epochs']} | \"\n",
        "              f\"Train Loss={tr_loss:.3f} PPL={tr_ppl:.2f} | \"\n",
        "              f\"Val Loss={va_loss:.3f} PPL={va_ppl:.2f}\")\n",
        "\n",
        "        if va_loss < best_val:\n",
        "            best_val = va_loss\n",
        "            torch.save(model.state_dict(), f\"{OUT_DIR}/{name}/best_model.pt\")\n",
        "\n",
        "    # Test\n",
        "    test_loss, test_ppl = evaluate(model, test_ld, crit)\n",
        "    print(f\"Final Test Loss={test_loss:.3f}, Test PPL={test_ppl:.2f}\")\n",
        "\n",
        "    # Save results\n",
        "    with open(f\"{OUT_DIR}/{name}/results.txt\", \"w\") as f:\n",
        "        f.write(f\"test_loss: {test_loss}\\n\")\n",
        "        f.write(f\"test_ppl: {test_ppl}\\n\")\n",
        "\n",
        "    # Plot\n",
        "    plt.figure()\n",
        "    plt.plot(history[\"train\"], label=\"train\")\n",
        "    plt.plot(history[\"val\"], label=\"val\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.title(name)\n",
        "    plt.legend()\n",
        "    plt.savefig(f\"{OUT_DIR}/{name}/loss.png\")\n",
        "    plt.close()\n",
        "\n",
        "# ---------------------------------------\n",
        "# EXPERIMENT CONFIGURATIONS\n",
        "# ---------------------------------------\n",
        "EXPS = {\n",
        "    \"underfit\": {\n",
        "        \"model\": \"lstm\",\n",
        "        \"emb\": 64,\n",
        "        \"hid\": 64,\n",
        "        \"layers\": 1,\n",
        "        \"lr\": 1e-3,\n",
        "        \"batch\": 64,\n",
        "        \"seq_len\": 20,\n",
        "        \"epochs\": 3,\n",
        "        \"min_freq\": 2,\n",
        "    },\n",
        "    \"overfit\": {\n",
        "        \"model\": \"lstm\",\n",
        "        \"emb\": 300,\n",
        "        \"hid\": 600,\n",
        "        \"layers\": 3,\n",
        "        \"lr\": 1e-4,\n",
        "        \"batch\": 16,\n",
        "        \"seq_len\": 50,\n",
        "        \"epochs\": 8,\n",
        "        \"min_freq\": 1,\n",
        "    },\n",
        "    \"best_fit\": {\n",
        "        \"model\": \"transformer\",\n",
        "        \"emb\": 256,\n",
        "        \"heads\": 8,\n",
        "        \"layers\": 4,\n",
        "        \"ff\": 512,\n",
        "        \"lr\": 5e-4,\n",
        "        \"batch\": 64,\n",
        "        \"seq_len\": 64,\n",
        "        \"epochs\": 5,\n",
        "        \"min_freq\": 2,\n",
        "    }\n",
        "}\n",
        "\n",
        "# ---------------------------------------\n",
        "# MAIN\n",
        "# ---------------------------------------\n",
        "def main():\n",
        "    os.makedirs(OUT_DIR, exist_ok=True)\n",
        "    for name, cfg in EXPS.items():\n",
        "        run_experiment(name, cfg)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EWGJnriN6OHo",
        "outputId": "4245fd0a-cfad-4846-f686-b5911964fc77"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "\n",
            "=== Running: underfit ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3 | Train Loss=5.728 PPL=307.47 | Val Loss=5.426 PPL=227.19\n",
            "Epoch 2/3 | Train Loss=4.865 PPL=129.71 | Val Loss=5.176 PPL=176.91\n",
            "Epoch 3/3 | Train Loss=4.485 PPL=88.68 | Val Loss=5.162 PPL=174.58\n",
            "Final Test Loss=5.903, Test PPL=366.31\n",
            "\n",
            "=== Running: overfit ===\n",
            "Epoch 1/8 | Train Loss=6.278 PPL=532.61 | Val Loss=6.245 PPL=515.64\n",
            "Epoch 2/8 | Train Loss=4.652 PPL=104.77 | Val Loss=6.866 PPL=959.07\n",
            "Epoch 3/8 | Train Loss=3.293 PPL=26.93 | Val Loss=8.476 PPL=4799.24\n",
            "Epoch 4/8 | Train Loss=2.348 PPL=10.46 | Val Loss=10.100 PPL=24344.78\n",
            "Epoch 5/8 | Train Loss=1.670 PPL=5.31 | Val Loss=11.595 PPL=108599.45\n",
            "Epoch 6/8 | Train Loss=1.185 PPL=3.27 | Val Loss=12.837 PPL=375734.56\n",
            "Epoch 7/8 | Train Loss=0.850 PPL=2.34 | Val Loss=13.742 PPL=929010.63\n",
            "Epoch 8/8 | Train Loss=0.634 PPL=1.88 | Val Loss=14.505 PPL=1992093.38\n",
            "Final Test Loss=16.166, Test PPL=10488910.38\n",
            "\n",
            "=== Running: best_fit ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5 | Train Loss=3.779 PPL=43.76 | Val Loss=5.879 PPL=357.47\n",
            "Epoch 2/5 | Train Loss=1.259 PPL=3.52 | Val Loss=7.892 PPL=2676.01\n",
            "Epoch 3/5 | Train Loss=0.623 PPL=1.86 | Val Loss=9.141 PPL=9326.82\n",
            "Epoch 4/5 | Train Loss=0.452 PPL=1.57 | Val Loss=10.010 PPL=22255.66\n",
            "Epoch 5/5 | Train Loss=0.377 PPL=1.46 | Val Loss=10.710 PPL=44779.86\n",
            "Final Test Loss=12.123, Test PPL=183987.56\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -R /content/nlm-assignment/outputs\n"
      ],
      "metadata": {
        "id": "BcJcuAZeO4R9",
        "outputId": "900fd0ef-4a57-4e2f-f8a2-9dc3d7d4007e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/nlm-assignment/outputs:\n",
            "best_fit  overfit  underfit\n",
            "\n",
            "/content/nlm-assignment/outputs/best_fit:\n",
            "best_model.pt  loss.png  results.txt\n",
            "\n",
            "/content/nlm-assignment/outputs/overfit:\n",
            "best_model.pt  loss.png  results.txt\n",
            "\n",
            "/content/nlm-assignment/outputs/underfit:\n",
            "best_model.pt  loss.png  results.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/nlm-assignment\n",
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MweccfgMRzBx",
        "outputId": "6cbc3928-856c-4ed5-f401-8ab0d0f84696"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/nlm-assignment\n",
            "data  outputs  Pride_and_Prejudice-Jane_Austen.txt  train_lm.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf /content/nlm-assignment/.git"
      ],
      "metadata": {
        "id": "FRtCSgQDac8W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content\n",
        "!zip -r nlm-assignment.zip nlm-assignment"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pHphZCtyahl2",
        "outputId": "2dbc21e7-aeb1-4c9c-b820-37c8c56c2966"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "updating: nlm-assignment/ (stored 0%)\n",
            "updating: nlm-assignment/outputs/ (stored 0%)\n",
            "updating: nlm-assignment/outputs/overfit/ (stored 0%)\n",
            "updating: nlm-assignment/outputs/overfit/results.txt (deflated 5%)\n",
            "updating: nlm-assignment/outputs/overfit/best_model.pt (deflated 7%)\n",
            "updating: nlm-assignment/outputs/overfit/loss.png (deflated 7%)\n",
            "updating: nlm-assignment/outputs/best_fit/ (stored 0%)\n",
            "updating: nlm-assignment/outputs/best_fit/results.txt (deflated 5%)\n",
            "updating: nlm-assignment/outputs/best_fit/best_model.pt (deflated 8%)\n",
            "updating: nlm-assignment/outputs/best_fit/loss.png (deflated 8%)\n",
            "updating: nlm-assignment/outputs/underfit/ (stored 0%)\n",
            "updating: nlm-assignment/outputs/underfit/results.txt (stored 0%)\n",
            "updating: nlm-assignment/outputs/underfit/best_model.pt (deflated 7%)\n",
            "updating: nlm-assignment/outputs/underfit/loss.png (deflated 13%)\n",
            "updating: nlm-assignment/Pride_and_Prejudice-Jane_Austen.txt (deflated 64%)\n",
            "updating: nlm-assignment/train_lm.py (stored 0%)\n",
            "updating: nlm-assignment/.ipynb_checkpoints/ (stored 0%)\n",
            "updating: nlm-assignment/data/ (stored 0%)\n",
            "updating: nlm-assignment/data/dataset.txt (deflated 64%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WyQhIh6HaxwR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}